{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.37.2 optimum==1.12.0 --quiet\n",
    "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --quiet\n",
    "!pip install langchain==0.1.9 --quiet\n",
    "# !pip install chromadb\n",
    "!pip install sentence_transformers==2.4.0 --quiet\n",
    "!pip install unstructured --quiet\n",
    "!pip install pdf2image --quiet\n",
    "!pip install pdfminer.six==20221105 --quiet\n",
    "!pip install unstructured-inference --quiet\n",
    "!pip install faiss-gpu==1.7.2 --quiet\n",
    "!pip install pikepdf==8.13.0 --quiet\n",
    "!pip install pypdf==4.0.2 --quiet\n",
    "!pip install pillow_heif==0.15.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "\n",
    "model_name = \"TheBloke/Llama-2-13b-Chat-GPTQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "gen_cfg = GenerationConfig.from_pretrained(model_name)\n",
    "gen_cfg.max_new_tokens=512\n",
    "gen_cfg.temperature=0.0000001 # 0.0\n",
    "gen_cfg.return_full_text=True\n",
    "gen_cfg.do_sample=True\n",
    "gen_cfg.repetition_penalty=1.11\n",
    "\n",
    "pipe=pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    generation_config=gen_cfg\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import fill\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "[INST] <>\n",
    "You are an AI assistant. You are truthful, unbiased and honest in your response.\n",
    "\n",
    "If you are unsure about an answer, truthfully say \"I don't know\"\n",
    "<>\n",
    "\n",
    "{text} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "#text = \"Explain artificial intelligence in a few lines\"\n",
    "#result = llm.invoke(prompt.format(text=text))\n",
    "#print(fill(result.strip(), width=100))\n",
    "text = \"Explain artificial intelligence in a few lines\"\n",
    "formatted_prompt = prompt.format(text=text)\n",
    "result = llm.invoke(formatted_prompt)\n",
    "\n",
    "# Extract the response text after the closing [/INST] tag\n",
    "response_start = result.find(\"[/INST]\") + len(\"[/INST]\")\n",
    "response = result[response_start:].strip()\n",
    "\n",
    "print(fill(response, width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
    "\n",
    "pdf_loader = UnstructuredPDFLoader(\"/content/The 2024 ICC Men's T20 World Cup.pdf\")\n",
    "pdf_doc = pdf_loader.load()\n",
    "updated_pdf_doc = filter_complex_metadata(pdf_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "chunked_pdf_doc = text_splitter.split_documents(updated_pdf_doc)\n",
    "len(chunked_pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the vectorized db with FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "db_pdf = FAISS.from_documents(chunked_pdf_doc, embeddings)\n",
    "\n",
    "# Create the vectorized db with Chroma\n",
    "# from langchain.vectorstores import Chroma\n",
    "# db_pdf = Chroma.from_documents(chunked_pdf_doc, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# use the recommended propt style for the LLAMA 2 LLM\n",
    "prompt_template = \"\"\"\n",
    "[INST] <>\n",
    "Use the following context to Answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
    "\n",
    "<>\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "#prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "Chain_pdf = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    # retriever=db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'k': 5, 'score_threshold': 0.8})\n",
    "    # Similarity Search is the default way to retrieve documents relevant to a query, but we can use MMR by setting search_type = \"mmr\"\n",
    "    # k defines how many documents are returned; defaults to 4.\n",
    "    # score_threshold allows to set a minimum relevance for documents returned by the retriever, if we are using the \"similarity_score_threshold\" search type.\n",
    "    # return_source_documents=True, # Optional parameter, returns the source documents used to answer the question\n",
    "    retriever=db_pdf.as_retriever(), # (search_kwargs={'k': 5, 'score_threshold': 0.8}),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "Chain_pdf = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db_pdf.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"which team is announced as team of tournament by ICC in t20 world cup 2024?\"\n",
    "result = Chain_pdf({\"query\": query})\n",
    "\n",
    "# Extract the response text after the closing [/INST] tag\n",
    "response_start = result['result'].find(\"[/INST]\") + len(\"[/INST]\")\n",
    "response = result['result'][response_start:].strip()\n",
    "\n",
    "print(fill(response, width=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask-cors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pyngrok import ngrok\n",
    "from flask_cors import CORS\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Define your prompt template and retrieval chain once\n",
    "prompt_template = \"\"\"\n",
    "[INST] <>\n",
    "Use the following context to answer the question at the end. Do not use any other information. If you can't find the relevant information in the context, just say you don't have enough information to answer the question. Don't try to make up an answer.\n",
    "\n",
    "<>\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "Chain_pdf = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db_pdf.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "@app.route('/ask', methods=['POST'])\n",
    "def ask():\n",
    "    data = request.json\n",
    "    query = data.get(\"query\")\n",
    "    result = Chain_pdf({\"query\": query})\n",
    "\n",
    "    response_start = result['result'].find(\"[/INST]\") + len(\"[/INST]\")\n",
    "    response = result['result'][response_start:].strip()\n",
    "\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    port = 5000\n",
    "\n",
    "    # Set ngrok authtoken\n",
    "    ngrok.set_auth_token(\"2iC5RBSZnGE9oAi1S9AJHFg4cxE_4EQbdAH37Yn8LhCZ2hzGc\")\n",
    "\n",
    "    public_url = ngrok.connect(port)\n",
    "    print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")\n",
    "    app.run(port=port)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
